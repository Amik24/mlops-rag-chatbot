import os
import re
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Chemin o√π download_data.py a d√©pos√© les fichiers
RAW_DATA_DIR = "data/raw"

def clean_text(text):
    """
    Nettoyage basique pour retirer les headers/footers inutiles des slides de cours.
    """
    # Retire les num√©ros de page isol√©s (ex: "PAGE 12")
    text = re.sub(r'PAGE \d+', '', text, flags=re.IGNORECASE)
    # Retire les ann√©es acad√©miques ou mentions r√©currentes
    text = re.sub(r'202\d-2\d', '', text)
    # Remplace les espaces multiples et sauts de ligne excessifs par un seul espace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def process_documents(data_dir=RAW_DATA_DIR):
    """
    Charge les PDFs, nettoie le texte et le d√©coupe en chunks.
    Retourne une liste de Documents LangChain.
    """
    documents = []
    
    # V√©rification de s√©curit√©
    if not os.path.exists(data_dir):
        print(f"‚ö†Ô∏è Attention : Le dossier {data_dir} n'existe pas.")
        return []

    # 1. Chargement des fichiers PDF
    files = [f for f in os.listdir(data_dir) if f.lower().endswith(".pdf")]
    
    if not files:
        print(f"‚ö†Ô∏è Aucun fichier PDF trouv√© dans {data_dir}")
        return []

    print(f"üìÑ Traitement de {len(files)} fichiers PDF...")

    for file in files:
        file_path = os.path.join(data_dir, file)
        try:
            # Utilisation du loader optimis√© de LangChain
            loader = PyPDFLoader(file_path)
            docs = loader.load()
            
            # Nettoyage page par page
            for doc in docs:
                doc.page_content = clean_text(doc.page_content)
                # On garde le nom du fichier source pour la citation des sources dans le RAG
                doc.metadata["source_file"] = file 
            
            documents.extend(docs)
            print(f"   ‚úÖ Charg√© : {file} ({len(docs)} pages)")
            
        except Exception as e:
            print(f"   ‚ùå Erreur lors de la lecture de {file}: {e}")

    # 2. Chunking (Segmentation)
    # On utilise une taille de 800 caract√®res avec un chevauchement de 100
    # C'est un bon √©quilibre pour capturer le contexte des slides de cours.
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=100,
        separators=["\n\n", "\n", ".", " ", ""]
    )
    
    chunks = text_splitter.split_documents(documents)
    print(f"‚úÇÔ∏è Segmentation termin√©e : {len(chunks)} chunks g√©n√©r√©s √† partir de {len(documents)} pages.")
    
    return chunks

if __name__ == "__main__":
    # Test local rapide
    chunks = process_documents()
    if chunks:
        print(f"Exemple de chunk : \n{chunks[0].page_content[:200]}...")
