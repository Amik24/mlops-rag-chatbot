import streamlit as st
import sys
import os
import boto3
from pathlib import Path

# --- Importation du module RAG (Syntaxe fiable de package) ---
from src.model.model_pipeline import RAGModel 

# Le reste de votre code...

# --- Fonctions Cl√©s pour le D√©ploiement Cloud ---

def setup_environment():
    """
    Lit les secrets Streamlit et les injecte dans os.environ.
    CRUCIAL pour que Boto3 et LangChain/Groq trouvent leurs cl√©s.
    """
    if not hasattr(st, 'secrets'):
        st.error("‚ùå Erreur : Les secrets Streamlit ne sont pas configur√©s. Arr√™t.")
        st.stop()
        return False
        
    try:
        # Cl√©s AWS pour Boto3 (utilis√©es dans RAGModel.load_model pour le t√©l√©chargement S3)
        os.environ["AWS_ACCESS_KEY_ID"] = st.secrets["AWS_ACCESS_KEY_ID"]
        os.environ["AWS_SECRET_ACCESS_KEY"] = st.secrets["AWS_SECRET_ACCESS_KEY"]
        os.environ["AWS_SESSION_TOKEN"] = st.secrets["AWS_SESSION_TOKEN"] 
        os.environ["AWS_REGION"] = st.secrets["AWS_REGION"] 
        
        # Cl√© Groq (LLM)
        os.environ["GROQ_API_KEY"] = st.secrets["GROQ_API_KEY"]
        
        print("‚úÖ Variables d'environnement configur√©es √† partir des secrets Streamlit.")
        return True
    except KeyError as e:
        st.error(f"‚ùå Erreur de configuration de secret : La cl√© {e} est manquante dans les Secrets Streamlit Cloud.")
        st.info("Veuillez v√©rifier que les secrets AWS et GROQ sont correctement d√©finis.")
        st.stop()
        return False


# --- Configuration et Initialisation Streamlit ---

st.set_page_config(page_title="Course RAG Bot", page_icon="üéì")

st.title("üéì MLOps Course Assistant")
st.markdown("Posez vos questions sur le NLP, le SVM, les RNNs, et les Transformers, bas√©es sur vos lectures.")

# 1. Configuration de l'environnement au d√©but
if not setup_environment():
    st.stop()


# 2. Initialisation du Mod√®le (avec t√©l√©chargement S3 int√©gr√© dans RAGModel.load_model())
if "rag" not in st.session_state:
    st.session_state.rag = RAGModel()
    
    with st.spinner("Chargement de la Base de Connaissances depuis S3..."):
        try:
            st.session_state.rag.load_model()
            st.success("‚úÖ Mod√®le RAG charg√© !")
        except FileNotFoundError as e:
             st.error(f"‚ùå Erreur critique : Index FAISS non trouv√©. Avez-vous ex√©cut√© le pipeline de vectorisation CI/CD ? D√©tail: {e}")
             st.session_state.rag.qa_chain = None 
        except Exception as e:
            st.error(f"‚ùå Erreur lors du chargement du mod√®le : {e}")
            st.session_state.rag.qa_chain = None 

# 3. Interface de Chat
if "messages" not in st.session_state:
    st.session_state.messages = []

# Afficher l'historique de chat
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Gestion de l'entr√©e utilisateur
if prompt := st.chat_input("Posez votre question ici... (ex: 'Qu'est-ce que le RAG ?'):"):
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        with st.spinner("R√©flexion en cours..."):
            
            if getattr(st.session_state.rag, 'qa_chain', None):
                answer, sources = st.session_state.rag.predict(prompt)
                
                # Formatage des sources
                sources_list = [f"**{src.split('/')[-1]}**" for src in sources]
                
                response_text = f"{answer}\n\n---\n\nüìö **Sources utilis√©es :** {', '.join(sources_list) if sources_list else 'Aucune source pertinente trouv√©e.'}"
                st.markdown(response_text)
            else:
                response_text = "D√©sol√©, le mod√®le n'a pas pu charger correctement. Veuillez v√©rifier les logs d'erreur."
                st.error(response_text)
            
    st.session_state.messages.append({"role": "assistant", "content": response_text})

